{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69bd81f4",
   "metadata": {},
   "source": [
    "# Revision of stats and ML for interview\n",
    "\n",
    "## Statistics\n",
    "\n",
    "### Distributions\n",
    "\n",
    "- Normal\n",
    "    - Xi-squared\n",
    "    - F distribution\n",
    "- Bernoulli\n",
    "- Binomial\n",
    "\n",
    "### Common questions:\n",
    "\n",
    "- What’s the difference between descriptive and inferential statistics\n",
    "    - Descriptive statistics provides some certain results that describe the data such as mean, median, st.dev, IQR, percentiles, curtosis\n",
    "        - Characteristics and distributions of the sample\n",
    "        - Doesn’t involve assumptions about the distribution\n",
    "        - It describes the sample\n",
    "    - Inferential statistics allows to test hypothesis (to infer from sample) about the population\n",
    "        - Often involves assumptions regarding the distribution\n",
    "- What Are the Main Measures Used to Describe the Central Tendency of Data?\n",
    "    - Mean, Median, Mode\n",
    "- Measures of variability\n",
    "    - Variance\n",
    "    - IQR\n",
    "    - Range\n",
    "    - St. dev\n",
    "- Skewness\n",
    "    - Measures of the symmetry of data\n",
    "    - Positive — right tail is longer\n",
    "    - Negative — left tail is longer\n",
    "- Kurtosis\n",
    "    - Measure of fat tails compared to the normal distribution\n",
    "- Correlation vs autocorrelation\n",
    "    - Correlation — measure of linear relationship between two variables\n",
    "    - Autocorrelation — measure of linear relationship between the same variable in consecutive periods (two values of the same variable)\n",
    "- Probability distribution vs sampling distribution\n",
    "    - Probability distribution — how any variable from a population is distributed\n",
    "    - Sampling distribution — distribution of a statistic that’s based on sample (sample mean, variance, etc)\n",
    "- Normal distribution\n",
    "    - Symmetric, bell shaped distribution that’s crucial due to the central limit theorem, based on mean and st.dev\n",
    "- Assumptions of linear regression\n",
    "    - Hoho, which ones\n",
    "    - MLR1 — linear in parameters\n",
    "    - MLR2 — Random sampling — iid\n",
    "    - MLR3 — no perfect collineariry\n",
    "    - MLR4 — Zero Conditional mean\n",
    "    - MLR5 — homoskedasticity\n",
    "    - MLR6 — normality\n",
    "- Hypothesis testing\n",
    "    - Evaluate hypothesis about a population\n",
    "    - H0, H1\n",
    "- Tests\n",
    "    - F test, t test, ANOVA\n",
    "- P value\n",
    "    - Probability of a type 1 error — probability that H0 is true given the sample. We reject h0 if p value is lower than significance level\n",
    "- Confidence interval\n",
    "    - Range we expect the result to lie with a significance level\n",
    "- LLN\n",
    "    - Sample mean → population\n",
    "- CLT\n",
    "    - I.i.d assumption\n",
    "    - Sampling distribution of mean is N(0, 1)\n",
    "- Probability vs likelihood\n",
    "    - Probability — chance of a particular outcome to occur\n",
    "    - Likelihood — measure to verify if parameters are trustworthy given result\n",
    "    \n",
    "\n",
    "# Machine Learning\n",
    "\n",
    "- Random forests\n",
    "    - Decision trees tend to overfit heavily\n",
    "    - We aggregate results of many decision trees to get good predictions by removing dependency on a particular set of features\n",
    "- Gradient boosting vs random forests\n",
    "    - Both are decision-tree based\n",
    "    - Random forest uses bagging — aggregating results of many trees\n",
    "    - Gradient boosting uses boosting — trees are arranged in a series sequential fashion, each tree trying to minimize error of the previous one\n",
    "    - Random forests — independent trees; gradient boosting — dependent on the previous\n",
    "    - Gradient boosting:\n",
    "        - More accurate since minimizing error\n",
    "        - Can capture complex patterns\n",
    "        - Better when used on unbalanced data sets\n",
    "        - Susceptible to overfitting\n",
    "        - More complex tuning of hyperparameters\n",
    "    - Random forests:\n",
    "        - Less prone to overfitting\n",
    "        - Has faster training since parallel trees\n",
    "- K-means clustering\n",
    "    - Partition dataset into K clusters, arbitrarily choose a centroid\n",
    "    - Repeatedly assigning points to the nearest centroid, update the centroid, repeating until convergence,\n",
    "    - Minimizes inner-cluster variance\n",
    "    - Elbow method for determining K — when stops to be a sharp decrease in variance\n",
    "- Dimensionality reduction\n",
    "    - Reduce dimensions of our data without sacrificing much variance / predictive power\n",
    "    - One popular method is Principal Components Analysis\n",
    "        - Combines highly correlated variables into a new smaller set , capturing most of the variance\n",
    "        - Looks for a linear combination to explain variance\n",
    "        - First component with the higher variance, second that is uncorrelated and has second-highest variance. Number depends on the threshold for percent of variance\n",
    "- L1, L2 regularization\n",
    "    - Regularization to reduce overfitting\n",
    "    - Normal cost function is MSE\n",
    "    - L1 — Lasso. Sum of absoulte of weights to the loss function we try to minimze\n",
    "    - L2 — ridge regression — sum of squares\n",
    "    - L1 helps as it tends to push parameters straight to zero, reather than holding them at some small level\n",
    "- Overfitting vs underfitting\n",
    "    - Overfitting — well on training, bad on test\n",
    "        - Learning patterns in data noice, bad in generalizing\n",
    "    - Underfitting — bad on both; too simple to learn any patterns\n",
    "    - Ways to avoid:\n",
    "        - Reduce number of features\n",
    "        - More representative training data\n",
    "        - Data preprocessing\n",
    "        - Regularization\n",
    "        - Techniques that tend to not ovefit\n",
    "        - Use validation set\n",
    "- Bias and variance\n",
    "    - Bias — simplifying assumption to make target easier to learn\n",
    "    - Variance — amount that the estimate will change if different training was used (variance error)\n",
    "    - Irreducible error\n",
    "    - Balance bias and variance\n",
    "    - Linear regression — high bias, low variance\n",
    "- Precision, recall, and F1\n",
    "    - Measures for evaluating classification\n",
    "    - Precision — ratio of correct predictions to A to total predictions to A (how likely a given prediction is correct)\n",
    "    - Recall — percentage of correctly classified of class A to  class A samples (how well can detect the class)\n",
    "    - Tradeoff between them\n",
    "    - F1 — harmonic mean of precision and recall. Used when both are equally important\n",
    "    - Choose what to optimise for based on the problem\n",
    "- Missing / corrupt data\n",
    "    - Deleting row with missing values\n",
    "    - Learning algorithms that support missing values\n",
    "        - K-NN (nearest neighbours)\n",
    "        - Naive Bayes\n",
    "        - Random forest — can work on non-linear / categorical data\n",
    "    - Imputation\n",
    "        - The most repeated value\n",
    "        - Mean-median-mode imputation\n",
    "            - Data leakage, doesn’t factor covariance\n",
    "        - Usee ML model to learn patter between data and predict the missing value\n",
    "- Robust to outliers\n",
    "    - Regularization\n",
    "    - Tree-based\n",
    "    - Transform (like log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
